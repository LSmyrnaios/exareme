---
- name: Find Leader
  # Maybe the nodes are unlabeled at this point, so use this type of hostname extraction.
  shell: kubectl get nodes -o json | jq --join-output '.items[] | select(.metadata.labels."node-role.kubernetes.io\/master") | .metadata.name'
  register: leader
  tags:
    - exareme


- name: Kill Kubernetes staff, but keep it initialized.
# Do not delete nodes.. nor reset the kubeadm.
  shell: kubectl delete daemonsets,replicasets,services,deployments,pods,rc,ing,statefulset,pv,pvc --all --force
  args:
    chdir: "{{ home_path }}"
  tags:
    - exareme


### Don't know if it's usefull.. and it seems to cause problems in pods..
#- name: Drain worker nodes
#  shell: >
#    kubectl drain {{ hostvars[item]['hostname'] }} --delete-local-data --force --ignore-daemonsets
#    && kubectl uncordon {{ hostvars[item]['hostname'] }}
#  with_items: "{{ groups['workers'] }}"
#  tags:
#    - exareme
#
#
#- name: Drain master node
#  shell: kubectl drain {{ leader.stdout }} --delete-local-data --force --ignore-daemonsets && kubectl uncordon {{ leader.stdout }}
#  tags:
#    - exareme


## TODO - Enable this when Dashboard support gets added.
#- name: Remove Dashboard
#  shell: kubectl delete namespace kubernetes-dashboard
#  ignore_errors: true
#  tags:
#    - dashboard
